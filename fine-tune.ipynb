{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":128376,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":108133,"modelId":132459}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning Code Llama for SQL Query Generation\n\nThis notebook demonstrates how to fine-tune the Code Llama model to become an expert SQL developer using the Hugging Face library. I will use the `b-mc2/sql-create-context` dataset, which contains text queries and their corresponding SQL queries. The approach involves using LoRA (Low-Rank Adaptation) to efficiently train the model by freezing its weights and only training an adapter. I will also quantize the model to int8 to optimize performance and use Weights & Biases (W&B) for experiment tracking. Let's get started!","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Set up the environment\n\nIn this step, I will set up the Kaggle environment by installing the necessary libraries. This includes the Hugging Face Transformers and Datasets libraries for model handling and dataset management, `bitsandbytes` for model quantization, and Weights & Biases (W&B) for tracking our experiments.","metadata":{}},{"cell_type":"code","source":"   !pip install transformers datasets accelerate\n   !pip install bitsandbytes\n   !pip install wandb\n   !pip install git+https://github.com/huggingface/peft.git@e536616888d51b453ed354a6f1e243fecb02ea08","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-08T10:21:47.049863Z","iopub.execute_input":"2024-10-08T10:21:47.050596Z","iopub.status.idle":"2024-10-08T10:22:54.196392Z","shell.execute_reply.started":"2024-10-08T10:21:47.050558Z","shell.execute_reply":"2024-10-08T10:22:54.195108Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nCollecting git+https://github.com/huggingface/peft.git@e536616888d51b453ed354a6f1e243fecb02ea08\n  Cloning https://github.com/huggingface/peft.git (to revision e536616888d51b453ed354a6f1e243fecb02ea08) to /tmp/pip-req-build-p2za2t_z\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-p2za2t_z\n  Running command git rev-parse -q --verify 'sha^e536616888d51b453ed354a6f1e243fecb02ea08'\n  Running command git fetch -q https://github.com/huggingface/peft.git e536616888d51b453ed354a6f1e243fecb02ea08\n  Running command git checkout -q e536616888d51b453ed354a6f1e243fecb02ea08\n  Resolved https://github.com/huggingface/peft.git to commit e536616888d51b453ed354a6f1e243fecb02ea08\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.3.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.3.0.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.3.0.dev0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.3.0.dev0) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.3.0.dev0) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.3.0.dev0) (4.45.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.3.0.dev0) (0.34.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.3.0.dev0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->peft==0.3.0.dev0) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate->peft==0.3.0.dev0) (0.4.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.3.0.dev0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.3.0.dev0) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.3.0.dev0) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.3.0.dev0) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.3.0.dev0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.3.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.3.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.3.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.3.0.dev0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.3.0.dev0) (1.3.0)\nBuilding wheels for collected packages: peft\n  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=41638 sha256=580eee682a5502d10b4eb80f43411a10eabdc10044493ae12100529ac3ec1dc8\n  Stored in directory: /root/.cache/pip/wheels/02/0a/9a/b9755f6b184f58a5a44baf58dd9664a49b3295e4b3e9a1f174\nSuccessfully built peft\nInstalling collected packages: peft\nSuccessfully installed peft-0.3.0.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nimport wandb\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom datasets import load_dataset\nfrom datetime import datetime\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForSeq2Seq)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n    PeftModel\n) ","metadata":{"execution":{"iopub.status.busy":"2024-10-08T10:22:54.198518Z","iopub.execute_input":"2024-10-08T10:22:54.198861Z","iopub.status.idle":"2024-10-08T10:23:14.225224Z","shell.execute_reply.started":"2024-10-08T10:22:54.198823Z","shell.execute_reply":"2024-10-08T10:23:14.224425Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Next, I will import the necessary libraries and initialize W&B for tracking our experiments. This will help us monitor the training process and evaluate the model's performance.","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient() \n\npersonal_key_for_api = user_secrets.get_secret(\"wandb-key\")\n\n! wandb login $personal_key_for_api","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:51:35.860965Z","iopub.execute_input":"2024-10-06T16:51:35.861413Z","iopub.status.idle":"2024-10-06T16:51:39.138862Z","shell.execute_reply.started":"2024-10-06T16:51:35.861373Z","shell.execute_reply":"2024-10-06T16:51:39.137617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Exploratory Data Analysis (EDA)\n\nBefore training the model, it's important to understand the dataset. I will load the `b-mc2/sql-create-context` dataset and perform exploratory data analysis (EDA) to gain insights into the data. This includes examining the structure of the dataset, analyzing the distribution of text and SQL query lengths, and visualizing these distributions.","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"b-mc2/sql-create-context\",split=\"train\")\ntrain_dataset = dataset.train_test_split(test_size=0.1)[\"train\"]\neval_dataset = dataset.train_test_split(test_size=0.1)[\"test\"]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:51:41.310997Z","iopub.execute_input":"2024-10-06T16:51:41.311866Z","iopub.status.idle":"2024-10-06T16:51:44.255379Z","shell.execute_reply.started":"2024-10-06T16:51:41.311801Z","shell.execute_reply":"2024-10-06T16:51:44.254635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:51:45.827240Z","iopub.execute_input":"2024-10-06T16:51:45.827955Z","iopub.status.idle":"2024-10-06T16:51:45.836467Z","shell.execute_reply.started":"2024-10-06T16:51:45.827913Z","shell.execute_reply":"2024-10-06T16:51:45.835519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:51:46.600633Z","iopub.execute_input":"2024-10-06T16:51:46.601009Z","iopub.status.idle":"2024-10-06T16:51:46.607107Z","shell.execute_reply.started":"2024-10-06T16:51:46.600975Z","shell.execute_reply":"2024-10-06T16:51:46.606133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:51:47.245644Z","iopub.execute_input":"2024-10-06T16:51:47.246283Z","iopub.status.idle":"2024-10-06T16:51:47.255079Z","shell.execute_reply.started":"2024-10-06T16:51:47.246242Z","shell.execute_reply":"2024-10-06T16:51:47.254094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature = \"answer\" # context, answer\n\nlengths = [len(sample[feature]) for sample in train_dataset]\nplt.hist(lengths, bins=50)\nplt.title(f'Distribution of {feature} lengths')\nplt.xlabel('Length')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:51:48.285722Z","iopub.execute_input":"2024-10-06T16:51:48.286414Z","iopub.status.idle":"2024-10-06T16:51:55.963794Z","shell.execute_reply.started":"2024-10-06T16:51:48.286371Z","shell.execute_reply":"2024-10-06T16:51:55.962747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Model preparation\n\nIn this step, I will prepare the Code Llama model for fine-tuning. I will load the pre-trained model and tokenizer, quantize the model to int8 using `bitsandbytes`, and freeze the model weights to train only the adapter. I will implement a LoRA adapter to efficiently adapt the model for SQL query generation.","metadata":{}},{"cell_type":"code","source":"model_name = \"codellama/CodeLlama-7b-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:51:55.965904Z","iopub.execute_input":"2024-10-06T16:51:55.966338Z","iopub.status.idle":"2024-10-06T16:53:13.191928Z","shell.execute_reply.started":"2024-10-06T16:51:55.966281Z","shell.execute_reply":"2024-10-06T16:53:13.191008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing the original model\n\nBefore fine-tunining the model, it is convenient to check the base model's ability to perform the task at hand.","metadata":{}},{"cell_type":"code","source":"eval_prompt = \"\"\"You are an SQL expert. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n\nYou must output the SQL query that answers the question.\n### Input:\nWhich Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n\n### Context:\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n\n### Response:\n\"\"\"\n# {'question': 'Name the comptroller for office of prohibition', 'context': 'CREATE TABLE table_22607062_1 (comptroller VARCHAR, ticket___office VARCHAR)', 'answer': 'SELECT comptroller FROM table_22607062_1 WHERE ticket___office = \"Prohibition\"'}\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\nwith torch.no_grad():\n    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:53:44.051623Z","iopub.execute_input":"2024-10-06T16:53:44.052063Z","iopub.status.idle":"2024-10-06T16:54:07.626772Z","shell.execute_reply.started":"2024-10-06T16:53:44.052027Z","shell.execute_reply":"2024-10-06T16:54:07.625640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This not the correct answer","metadata":{}},{"cell_type":"code","source":"model.train()\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = prepare_model_for_int8_training(model)\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:54:15.761235Z","iopub.execute_input":"2024-10-06T16:54:15.762026Z","iopub.status.idle":"2024-10-06T16:54:16.096773Z","shell.execute_reply.started":"2024-10-06T16:54:15.761986Z","shell.execute_reply":"2024-10-06T16:54:16.095729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Training\n\nWith the model prepared, I will now proceed to train it on the dataset. I will tokenize the dataset, set up training arguments, and use the Hugging Face Trainer to fine-tune the model.","metadata":{}},{"cell_type":"code","source":"tokenizer.add_eos_token = True\ntokenizer.pad_token_id = 0\ntokenizer.padding_side = \"left\"","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:54:20.536225Z","iopub.execute_input":"2024-10-06T16:54:20.537042Z","iopub.status.idle":"2024-10-06T16:54:20.541407Z","shell.execute_reply.started":"2024-10-06T16:54:20.537001Z","shell.execute_reply":"2024-10-06T16:54:20.540431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(prompt):\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=512,\n        padding=False,\n        return_tensors=None,\n    )\n\n    # self-supervised learning\" means the labels are also the inputs:\n    result[\"labels\"] = result[\"input_ids\"].copy()\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:54:22.185974Z","iopub.execute_input":"2024-10-06T16:54:22.187061Z","iopub.status.idle":"2024-10-06T16:54:22.191805Z","shell.execute_reply.started":"2024-10-06T16:54:22.187018Z","shell.execute_reply":"2024-10-06T16:54:22.190955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_and_tokenize_prompt(data_point):\n    full_prompt =f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n\nYou must output the SQL query that answers the question.\n\n### Input:\n{data_point[\"question\"]}\n\n### Context:\n{data_point[\"context\"]}\n\n### Response:\n{data_point[\"answer\"]}\n\"\"\"\n    return tokenize_function(full_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:54:25.061239Z","iopub.execute_input":"2024-10-06T16:54:25.062250Z","iopub.status.idle":"2024-10-06T16:54:25.067145Z","shell.execute_reply.started":"2024-10-06T16:54:25.062207Z","shell.execute_reply":"2024-10-06T16:54:25.066176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\ntokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:54:27.090552Z","iopub.execute_input":"2024-10-06T16:54:27.090945Z","iopub.status.idle":"2024-10-06T16:55:21.270092Z","shell.execute_reply.started":"2024-10-06T16:54:27.090907Z","shell.execute_reply":"2024-10-06T16:55:21.269023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = \"sql-code-llama\"\n\ntraining_args = TrainingArguments(\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        warmup_steps=100,\n        max_steps=400,\n        learning_rate=3e-4,\n        fp16=True,\n        logging_steps=1,\n        optim=\"adamw_torch\",\n        evaluation_strategy=\"steps\", # if val_set_size > 0 else \"no\",\n        save_strategy=\"steps\",\n        eval_steps=20,\n        save_steps=20,\n        output_dir=output_dir,\n        # save_total_limit=3,\n        load_best_model_at_end=False,\n        # ddp_find_unused_parameters=False if ddp else None,\n        group_by_length=True, # group sequences of roughly the same length together to speed up training\n        report_to=\"wandb\", # if use_wandb else \"none\",\n        run_name=f\"codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\", # if use_wandb else None,\n    )\nmodel.config.use_cache = False \n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:56:07.418029Z","iopub.execute_input":"2024-10-06T16:56:07.418793Z","iopub.status.idle":"2024-10-06T16:56:07.468230Z","shell.execute_reply.started":"2024-10-06T16:56:07.418740Z","shell.execute_reply":"2024-10-06T16:56:07.467131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    args=training_args,\n    data_collator=DataCollatorForSeq2Seq(\n        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:56:20.103273Z","iopub.execute_input":"2024-10-06T16:56:20.103818Z","iopub.status.idle":"2024-10-06T16:56:20.138073Z","shell.execute_reply.started":"2024-10-06T16:56:20.103765Z","shell.execute_reply":"2024-10-06T16:56:20.137187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()\n\ntrainer.model.save_pretrained(\"trained-model\")\ntokenizer.save_pretrained(\"tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:56:22.036270Z","iopub.execute_input":"2024-10-06T16:56:22.037023Z","iopub.status.idle":"2024-10-06T16:57:38.848163Z","shell.execute_reply.started":"2024-10-06T16:56:22.036981Z","shell.execute_reply":"2024-10-06T16:57:38.846502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Evaluation\n\nI will now load the trained model for inference","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = \"codellama/CodeLlama-7b-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T10:23:14.226397Z","iopub.execute_input":"2024-10-08T10:23:14.227038Z","iopub.status.idle":"2024-10-08T10:24:48.436089Z","shell.execute_reply.started":"2024-10-08T10:23:14.227002Z","shell.execute_reply":"2024-10-08T10:24:48.435052Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"562e5c4e5e134c809d039333fe9e10a4"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3bea88b6464510b1451c56dc62f338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b13274af8264bacb187322fe8ba6243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa5effa7ea3b4c7d8d572e62c6519e64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b448881f6046be8902aa20b1b15212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"056f8f88a8564c37abb8f1525c36b841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c53f5d9fc3d94f048696294a6996d830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d2f856820844ce3be801db4289bac15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7af457c87160446494eefba074bd899a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf78bf7dc291459898fc34f5b9a04fc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9307c3fac14e5cb1b12f0f83b5a91f"}},"metadata":{}}]},{"cell_type":"code","source":"output_dir = \"/kaggle/input/code-llama-sql/transformers/default/1/sql-code-llama/checkpoint-400\"\nmodel = PeftModel.from_pretrained(model, output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T10:24:48.438220Z","iopub.execute_input":"2024-10-08T10:24:48.438608Z","iopub.status.idle":"2024-10-08T10:24:49.697278Z","shell.execute_reply.started":"2024-10-08T10:24:48.438566Z","shell.execute_reply":"2024-10-08T10:24:49.696499Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:159: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  adapters_weights = torch.load(\n","output_type":"stream"}]},{"cell_type":"code","source":"#for i in model.named_parameters():\n#    print(f\"{i[0]} -> {i[1].device}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-08T10:15:36.864148Z","iopub.execute_input":"2024-10-08T10:15:36.864447Z","iopub.status.idle":"2024-10-08T10:15:39.533827Z","shell.execute_reply.started":"2024-10-08T10:15:36.864415Z","shell.execute_reply":"2024-10-08T10:15:39.532684Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def postprocess_model_output(model_output: str) -> str:\n    \"\"\"\n    Extracts the SQL query from the model's output by finding the text after the '### Response:' marker.\n\n    Parameters:\n    - model_output (str): The full output string from the model.\n\n    Returns:\n    - str: The extracted SQL query.\n    \"\"\"\n    # Define the marker that indicates the start of the response\n    response_marker = \"### Response:\"\n\n    # Find the position of the response marker\n    response_start = model_output.find(response_marker)\n\n    # If the marker is found, extract the text following it\n    if response_start != -1:\n      # Extract the response text, stripping any leading/trailing whitespace\n        response_text = model_output[response_start + len(response_marker):].strip()\n        return response_text\n    else:\n      # If the marker is not found, return an empty string or handle as needed\n        return \"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-08T10:29:25.606200Z","iopub.execute_input":"2024-10-08T10:29:25.607092Z","iopub.status.idle":"2024-10-08T10:29:25.615069Z","shell.execute_reply.started":"2024-10-08T10:29:25.607026Z","shell.execute_reply":"2024-10-08T10:29:25.614041Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"eval_prompt = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n\nYou must output the SQL query that answers the question.\n### Input:\nWhich Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n\n### Context:\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n\n### Response:\n\"\"\"\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    print(postprocess_model_output(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True)))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T10:28:43.640546Z","iopub.execute_input":"2024-10-08T10:28:43.640969Z","iopub.status.idle":"2024-10-08T10:28:54.737492Z","shell.execute_reply.started":"2024-10-08T10:28:43.640930Z","shell.execute_reply":"2024-10-08T10:28:54.736469Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"SELECT class FROM table_name_12 WHERE frequency_mhz > 91.5 AND city_of_license = \"hyannis, nebraska\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The model now outputs the correct answer.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook, I successfully fine-tuned the Code Llama model to generate SQL queries using the `b-mc2/sql-create-context` dataset. I utilized a LoRA approach to efficiently train the model by freezing its weights and only training an adapter. The model was quantized to int8 for optimized performance, and I tracked our experiments using Weights & Biases. This setup provides a robust framework for transforming Code Llama into a proficient SQL developer model.","metadata":{}}]}
